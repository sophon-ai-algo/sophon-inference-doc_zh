AutoRunner
==========

AutoRunner only supports FP32 by now.

Once you use AutoSplit to split and compile model, you can do inference through AutoRunner,
without worrying about how to write the program to drive bmodel.
The AutoRunner source codes are located in the "module/auto_runner/" directory and currently supports tensorflow/mxnet.
We provide two functions in "auto_runner.api", which are:

* load:

Parse the "graph_infos.json" file in the specified directory,
and load the submodels sequentially, return a Runner instance.

* inferï¼š

Use given Runner instance and input tensors to do inference sequentially.

Usually AutoRunner needs to be used with AutoSplit,
but users can also manually split and compile the model,
as long as it is saved as specified in graph_infos.json.

load
____

This function can load the splitted submodels based on the information in the "graph_infos.json" file in the specified directory after splitting.
The main function is implemented by the Runner class.
**Runner** sequentially loads the submodels in the graphs list in "graph_infos.json" at initialization time. 
If the device attribute of the submodel is cpu, 
then the related api in the original deep learning framework is called, 
and the model is loaded according to the model_info attribute;
If the device attribute of the submodel is tpu, 
then SAIL's engine class is called, and the model is loaded according to the context_dir attribute.

Description of fucntions is as follows.
For details, refer to modules: **auto_deploy.common.base_runner** and **auto_deploy.runner**.

    .. code-block:: python

       @exception_wrapper(message="Met an Error when load model.")
       def load(folder, graphs_in_memory=None):
         """ load all the subgraphs in a runner.
       
         Args:
           folder: Directory path that contains splitted models and 'graph_infos.json'.
       
           Format of 'graph_infos.json':
           {
             "graph_num": graph_numbmer,
             "platform": "mxnet"
             "layout": "NCHW"
             "dynamic": False
             "graphs": [
               {
                 "device": "cpu",
                 "inputs": list of input tensor names,
                 "outputs": list of output tensor names,
                 "model_info": {
                   "json": json_file_path,
                   "params": params_file_path
                 }
               }
             ]
             "tensors": [
               {
                 "name": name,
                 "shape": list for shape,
                 "attr": "input" or "output" or "intermediate"
               }
             ]
           }
           input_tensors: Input tensors. Format: {name: value}.
           graphs_in_memory: A dict of graphs which already read in memory.
       
         Returns:
           An instance of Runner.
         """

infer
_____

    .. code-block:: python

       @exception_wrapper(message="Met an Error when infer loaded models.")
       def infer(runner, input_tensors):
         """ Run loaded subgraphs.
       
         Args:
           runner: an instance of Runner.
           input_tensors: Input tensors. Format: {name: value}.
       
         Returns:
           Output tensors. Format: {name: value}.
         """


